{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b41589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "     ---------------------------------------- 0.0/246.5 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 112.6/246.5 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 246.5/246.5 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\documents\\data_science_course\\anaconda\\lib\\site-packages (1.5.3)\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\hp\\documents\\data_science_course\\anaconda\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\documents\\data_science_course\\anaconda\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\documents\\data_science_course\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\documents\\data_science_course\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Building wheels for collected packages: avro-python3\n",
      "  Building wheel for avro-python3 (setup.py): started\n",
      "  Building wheel for avro-python3 (setup.py): finished with status 'done'\n",
      "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44035 sha256=6b4edd808c083a4f34d1a2d8fab47baaa7ddbd65ff52f49a28bd785356a76344\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\5a\\29\\4d\\510c0e098c49c5e49519f430481a5425e60b8752682d7b1e55\n",
      "Successfully built avro-python3\n",
      "Installing collected packages: kafka-python, avro-python3\n",
      "Successfully installed avro-python3-1.10.2 kafka-python-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python pandas avro-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c664af3",
   "metadata": {},
   "source": [
    "## Data Ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0302b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "# Initialize Kafka producers\n",
    "producer_json = KafkaProducer(bootstrap_servers=bootstrap_servers,\n",
    "                              value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "producer_csv = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "# Define topics for different data sources\n",
    "topic_ad_impressions = 'ad_impressions'\n",
    "topic_clicks_conversions = 'clicks_conversions'\n",
    "topic_bid_requests = 'bid_requests'\n",
    "\n",
    "# Function to publish JSON data to Kafka\n",
    "def publish_json_data(topic, data):\n",
    "    producer_json.send(topic, value=data)\n",
    "    producer_json.flush()\n",
    "\n",
    "# Function to publish CSV data to Kafka\n",
    "def publish_csv_data(topic, data):\n",
    "    producer_csv.send(topic, value=data.encode('utf-8'))\n",
    "    producer_csv.flush()\n",
    "\n",
    "# Function to read CSV file and publish data to Kafka\n",
    "def publish_csv_file_to_kafka(file_path, topic):\n",
    "    with open(file_path, 'r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        for row in csv_reader:\n",
    "            publish_csv_data(topic, ','.join(row.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "# Kafka configuration\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer_avro = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "# Define topic for bid requests\n",
    "topic_bid_requests = 'bid_requests'\n",
    "\n",
    "# Function to publish Avro data to Kafka\n",
    "def publish_avro_data(topic, data):\n",
    "    producer_avro.send(topic, value=data)\n",
    "    producer_avro.flush()\n",
    "\n",
    "# Function to read Avro file and publish data to Kafka\n",
    "def publish_avro_file_to_kafka(file_path, topic):\n",
    "    with open(file_path, 'rb') as avro_file:\n",
    "        reader = DataFileReader(avro_file, DatumReader())\n",
    "        for record in reader:\n",
    "            publish_avro_data(topic, record)\n",
    "\n",
    "# Example usage:\n",
    "avro_file_path = r\"C:\\Users\\HP\\Downloads\\userdata5.avro\"\n",
    "publish_avro_file_to_kafka(avro_file_path, topic_bid_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac56af",
   "metadata": {},
   "source": [
    "## Data Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39939eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "\n",
    "# Function to consume data from Kafka topic\n",
    "def consume_data_from_kafka(topic):\n",
    "    consumer = KafkaConsumer(topic, bootstrap_servers=bootstrap_servers,\n",
    "                             value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n",
    "    for message in consumer:\n",
    "        yield message.value\n",
    "\n",
    "# Consume ad impressions data from Kafka\n",
    "ad_impressions_data = consume_data_from_kafka('ad_impressions')\n",
    "ad_impressions_df = pd.DataFrame(ad_impressions_data)\n",
    "\n",
    "# Consume clicks/conversions data from Kafka\n",
    "clicks_conversions_data = consume_data_from_kafka('clicks_conversions')\n",
    "clicks_conversions_df = pd.DataFrame(clicks_conversions_data)\n",
    "\n",
    "# Consume bid requests data from Kafka\n",
    "bid_requests_data = consume_data_from_kafka('bid_requests')\n",
    "# Assuming bid requests data is in JSON format\n",
    "bid_requests_df = pd.DataFrame(bid_requests_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "\n",
    "# Merge ad impressions with clicks/conversions\n",
    "merged_data = pd.merge(ad_impressions_df, clicks_conversions_df, on='user_id', how='left')\n",
    "\n",
    "# Filter out invalid data\n",
    "valid_data = merged_data.dropna(subset=['conversion_type'])\n",
    "\n",
    "# Deduplicate data\n",
    "deduplicated_data = valid_data.drop_duplicates()\n",
    "\n",
    "# Correlate ad impressions with clicks/conversions\n",
    "correlation_results = deduplicated_data.groupby(['ad_id', 'conversion_type']).size().reset_index(name='counts')\n",
    "\n",
    "# Display processed data\n",
    "print(\"Processed Data:\")\n",
    "print(correlation_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69329027",
   "metadata": {},
   "source": [
    "## Data Storage & Query Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaec6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# Fetch Redshift cluster details from environment variables\n",
    "redshift_cluster_endpoint = os.environ.get('REDSHIFT_CLUSTER_ENDPOINT')\n",
    "redshift_port = os.environ.get('REDSHIFT_PORT')\n",
    "redshift_dbname = os.environ.get('REDSHIFT_DBNAME')\n",
    "redshift_username = os.environ.get('REDSHIFT_USERNAME')\n",
    "redshift_password = os.environ.get('REDSHIFT_PASSWORD')\n",
    "\n",
    "# Ensure all required environment variables are set\n",
    "if not all([redshift_cluster_endpoint, redshift_port, redshift_dbname, redshift_username, redshift_password]):\n",
    "    raise ValueError(\"One or more Redshift environment variables are not set\")\n",
    "\n",
    "# Connect to Redshift cluster\n",
    "conn = psycopg2.connect(\n",
    "    host=redshift_cluster_endpoint,\n",
    "    port=redshift_port,\n",
    "    dbname=redshift_dbname,\n",
    "    user=redshift_username,\n",
    "    password=redshift_password\n",
    ")\n",
    "\n",
    "# Create tables in Redshift\n",
    "create_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ad_campaign_data (\n",
    "    ad_id INT,\n",
    "    conversion_type VARCHAR(100),\n",
    "    counts INT\n",
    ")\n",
    "\"\"\"\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(create_table_query)\n",
    "conn.commit()\n",
    "\n",
    "# Load processed data into Redshift \n",
    "copy_query = \"\"\"\n",
    "COPY ad_campaign_data FROM 's3://your-s3-bucket/.hidden-folder/processed_data.csv'\n",
    "CREDENTIALS 'aws_access_key_id=your-access-key;aws_secret_access_key=your-secret-key'\n",
    "CSV;\n",
    "\"\"\"\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(copy_query)\n",
    "conn.commit()\n",
    "\n",
    "# Perform analytical queries\n",
    "query = \"\"\"\n",
    "SELECT ad_id, conversion_type, SUM(counts) AS total_count\n",
    "FROM ad_campaign_data\n",
    "GROUP BY ad_id, conversion_type\n",
    "ORDER BY total_count DESC;\n",
    "\"\"\"\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(query)\n",
    "    rows = cur.fetchall()\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "\n",
    "# Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccde6bb",
   "metadata": {},
   "source": [
    "## Error Handling and Monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91712bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='data_pipeline.log', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# function for data processing\n",
    "def process_data(data):\n",
    "    try:\n",
    "        # data processing logic\n",
    "        processed_data = data * 2\n",
    "        return processed_data\n",
    "    except Exception as e:\n",
    "        # Log error and raise exception\n",
    "        logging.error(f\"Error processing data: {e}\")\n",
    "        raise\n",
    "\n",
    "# usage of data processing function\n",
    "try:\n",
    "    input_data = 10\n",
    "    output_data = process_data(input_data)\n",
    "    print(\"Processed data:\", output_data)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "import random\n",
    "# for monitoring\n",
    "def monitor_pipeline():\n",
    "    # Simulate retrieving performance metrics\n",
    "    throughput = random.randint(50, 100)  # throughput in messages per second\n",
    "    latency = random.uniform(0.1, 0.5)     # latency in seconds\n",
    "\n",
    "    # Check for anomalies or deviations from expected behavior\n",
    "    if throughput < 70:\n",
    "        send_alert(f\"Low throughput detected: {throughput} messages per second\")\n",
    "    if latency > 0.3:\n",
    "        send_alert(f\"High latency detected: {latency} seconds\")\n",
    "\n",
    "    # Log metrics\n",
    "    print(f\"Throughput: {throughput} messages per second\")\n",
    "    print(f\"Latency: {latency} seconds\")\n",
    "\n",
    "# for alerting mechanism\n",
    "def send_alert(message):\n",
    "    # Simulate sending alert notification\n",
    "    print(\"Alert:\", message)\n",
    "\n",
    "\n",
    "# usage of alerting mechanism\n",
    "try:\n",
    "    # Monitor pipeline and trigger alerts if needed\n",
    "    monitor_pipeline()\n",
    "except Exception as e:\n",
    "    # Send alert for pipeline failure\n",
    "    send_alert(f\"Data pipeline failure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfc90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e15b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
